{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZJOBskfsllR2QcwRBGQTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soheldatta17/Error-Detection-English-Grammar/blob/main/Error_Detection_English_Grammar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pandas numpy scikit-learn\n"
      ],
      "metadata": {
        "id": "PJeMUn6LGb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/misspelled.csv')\n",
        "\n",
        "# Drop the index column if not needed\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Use rows 0 to 600 (index starts from 0, so it's 0 to 599)\n",
        "data_subset = data.iloc[0:600]\n",
        "\n",
        "# Handle missing values: fill missing values with an empty string\n",
        "data_subset['input'] = data_subset['input'].fillna('')\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_subset['input'])\n",
        "X = tokenizer.texts_to_sequences(data_subset['input'])\n",
        "X = pad_sequences(X)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data_subset['label'])\n",
        "y = to_categorical(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Neural Network model with enhanced architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256))  # Increased embedding dimension\n",
        "model.add(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))  # Increased LSTM units\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  # Increased LSTM units\n",
        "model.add(Dense(128, activation='relu'))  # Added dense layer\n",
        "model.add(Dropout(0.5))  # Dropout layer for regularization\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 50 epochs to ensure thorough training\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "def correct_spelling(misspelled_word):\n",
        "    # Vectorize the input word\n",
        "    word_sequence = tokenizer.texts_to_sequences([misspelled_word])\n",
        "    word_sequence = pad_sequences(word_sequence, maxlen=X.shape[1])\n",
        "\n",
        "    # Predict the correct word\n",
        "    predicted_label = model.predict(word_sequence)\n",
        "    correct_word = label_encoder.inverse_transform([np.argmax(predicted_label)])\n",
        "\n",
        "    return correct_word[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGu6UBR9HQ19",
        "outputId": "35a2f0b2-2ed6-4fc0-fe31-3bd09f3aa255"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-feea2bd7e2d2>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_subset['input'] = data_subset['input'].fillna('')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.0000e+00 - loss: 6.2074 - val_accuracy: 0.0000e+00 - val_loss: 6.2128\n",
            "Epoch 2/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.0082 - loss: 6.2019 - val_accuracy: 0.0000e+00 - val_loss: 6.2242\n",
            "Epoch 3/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0038 - loss: 6.1951 - val_accuracy: 0.0000e+00 - val_loss: 6.2417\n",
            "Epoch 4/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.0061 - loss: 6.1823 - val_accuracy: 0.0000e+00 - val_loss: 6.2845\n",
            "Epoch 5/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.0013 - loss: 6.1530 - val_accuracy: 0.0000e+00 - val_loss: 6.4588\n",
            "Epoch 6/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.0052 - loss: 6.0258 - val_accuracy: 0.0000e+00 - val_loss: 7.2531\n",
            "Epoch 7/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.0145 - loss: 5.8350 - val_accuracy: 0.0000e+00 - val_loss: 7.4724\n",
            "Epoch 8/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.0013 - loss: 5.7651 - val_accuracy: 0.0000e+00 - val_loss: 7.2515\n",
            "Epoch 9/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.0102 - loss: 5.6817 - val_accuracy: 0.0000e+00 - val_loss: 7.4812\n",
            "Epoch 10/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0144 - loss: 5.6268 - val_accuracy: 0.0000e+00 - val_loss: 7.5510\n",
            "Epoch 11/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0138 - loss: 5.6257 - val_accuracy: 0.0000e+00 - val_loss: 7.6583\n",
            "Epoch 12/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0176 - loss: 5.4413 - val_accuracy: 0.0000e+00 - val_loss: 7.7309\n",
            "Epoch 13/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0160 - loss: 5.4264 - val_accuracy: 0.0000e+00 - val_loss: 7.6867\n",
            "Epoch 14/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0104 - loss: 5.2808 - val_accuracy: 0.0000e+00 - val_loss: 7.9460\n",
            "Epoch 15/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0064 - loss: 5.0831 - val_accuracy: 0.0000e+00 - val_loss: 7.9327\n",
            "Epoch 16/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.0403 - loss: 5.0234 - val_accuracy: 0.0000e+00 - val_loss: 8.1878\n",
            "Epoch 17/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0299 - loss: 4.8665 - val_accuracy: 0.0000e+00 - val_loss: 8.4080\n",
            "Epoch 18/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.0397 - loss: 4.6965 - val_accuracy: 0.0000e+00 - val_loss: 8.6181\n",
            "Epoch 19/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.0657 - loss: 4.4643 - val_accuracy: 0.0000e+00 - val_loss: 8.9431\n",
            "Epoch 20/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.0575 - loss: 4.3103 - val_accuracy: 0.0000e+00 - val_loss: 9.3322\n",
            "Epoch 21/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.0905 - loss: 4.0680 - val_accuracy: 0.0000e+00 - val_loss: 9.4062\n",
            "Epoch 22/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.1549 - loss: 3.8229 - val_accuracy: 0.0000e+00 - val_loss: 9.9506\n",
            "Epoch 23/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.1145 - loss: 3.7360 - val_accuracy: 0.0000e+00 - val_loss: 9.9151\n",
            "Epoch 24/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.1456 - loss: 3.3668 - val_accuracy: 0.0000e+00 - val_loss: 10.8568\n",
            "Epoch 25/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2286 - loss: 3.1652 - val_accuracy: 0.0000e+00 - val_loss: 10.7842\n",
            "Epoch 26/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.2105 - loss: 3.1116 - val_accuracy: 0.0000e+00 - val_loss: 11.2202\n",
            "Epoch 27/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.2829 - loss: 2.8287 - val_accuracy: 0.0000e+00 - val_loss: 11.5631\n",
            "Epoch 28/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2911 - loss: 2.6621 - val_accuracy: 0.0000e+00 - val_loss: 11.9664\n",
            "Epoch 29/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.3353 - loss: 2.4931 - val_accuracy: 0.0000e+00 - val_loss: 12.2996\n",
            "Epoch 30/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.3791 - loss: 2.3047 - val_accuracy: 0.0000e+00 - val_loss: 12.6357\n",
            "Epoch 31/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3581 - loss: 2.3356 - val_accuracy: 0.0000e+00 - val_loss: 12.5368\n",
            "Epoch 32/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.4365 - loss: 2.0713 - val_accuracy: 0.0000e+00 - val_loss: 12.7489\n",
            "Epoch 33/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.4436 - loss: 1.9818 - val_accuracy: 0.0000e+00 - val_loss: 12.6445\n",
            "Epoch 34/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3831 - loss: 2.0314 - val_accuracy: 0.0000e+00 - val_loss: 12.6927\n",
            "Epoch 35/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3944 - loss: 1.9738 - val_accuracy: 0.0000e+00 - val_loss: 13.2482\n",
            "Epoch 36/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4717 - loss: 1.8237 - val_accuracy: 0.0000e+00 - val_loss: 12.8829\n",
            "Epoch 37/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4405 - loss: 1.8334 - val_accuracy: 0.0000e+00 - val_loss: 13.2519\n",
            "Epoch 38/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5284 - loss: 1.6555 - val_accuracy: 0.0000e+00 - val_loss: 12.8711\n",
            "Epoch 39/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5173 - loss: 1.5432 - val_accuracy: 0.0000e+00 - val_loss: 13.2936\n",
            "Epoch 40/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5160 - loss: 1.5722 - val_accuracy: 0.0000e+00 - val_loss: 13.6684\n",
            "Epoch 41/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5368 - loss: 1.5033 - val_accuracy: 0.0000e+00 - val_loss: 13.4019\n",
            "Epoch 42/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5489 - loss: 1.4258 - val_accuracy: 0.0000e+00 - val_loss: 13.9465\n",
            "Epoch 43/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5930 - loss: 1.3747 - val_accuracy: 0.0000e+00 - val_loss: 13.5005\n",
            "Epoch 44/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6504 - loss: 1.2784 - val_accuracy: 0.0000e+00 - val_loss: 14.1875\n",
            "Epoch 45/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6501 - loss: 1.2140 - val_accuracy: 0.0000e+00 - val_loss: 14.1070\n",
            "Epoch 46/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6790 - loss: 1.1392 - val_accuracy: 0.0000e+00 - val_loss: 14.1041\n",
            "Epoch 47/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6466 - loss: 1.2151 - val_accuracy: 0.0000e+00 - val_loss: 14.3675\n",
            "Epoch 48/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6878 - loss: 1.0451 - val_accuracy: 0.0000e+00 - val_loss: 14.3448\n",
            "Epoch 49/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6783 - loss: 1.0711 - val_accuracy: 0.0000e+00 - val_loss: 14.3186\n",
            "Epoch 50/50\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.6817 - loss: 1.0112 - val_accuracy: 0.0000e+00 - val_loss: 14.7707\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 14.4545 \n",
            "Accuracy: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misspelled_words = [\n",
        "    'accomodate',\n",
        "    'acheive',\n",
        "    'arguement',\n",
        "    'benefical',\n",
        "    'beleive',\n",
        "    'buisness',\n",
        "    'calender',\n",
        "    'catergory',\n",
        "    'cemetary'\n",
        "]\n",
        "\n",
        "for i in misspelled_words:\n",
        "  print(f'Misspelled: {i}')\n",
        "  print(f'Corrected: {correct_spelling(i)}')\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a71CD-UIHnx",
        "outputId": "f4184983-205e-4947-e6a7-cd2d4c27a0a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misspelled: accomodate\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Corrected: accommodate\n",
            "\n",
            "Misspelled: acheive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Corrected: achieve\n",
            "\n",
            "Misspelled: arguement\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Corrected: bankruptcy\n",
            "\n",
            "Misspelled: benefical\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Corrected: bankruptcy\n",
            "\n",
            "Misspelled: beleive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Corrected: believe\n",
            "\n",
            "Misspelled: buisness\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Corrected: business\n",
            "\n",
            "Misspelled: calender\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Corrected: calendar\n",
            "\n",
            "Misspelled: catergory\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Corrected: bankruptcy\n",
            "\n",
            "Misspelled: cemetary\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Corrected: almost\n",
            "\n"
          ]
        }
      ]
    }
  ]
}